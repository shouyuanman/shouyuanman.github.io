---
title: 白话BGE
date: 2025-06-15 22:19:00 +0800
categories: [AI, NLP]
tags: [AI, NLP, LLM, BGE]
music-id: 2620072586
---

## **Overview**
`BGE`，`BAAI General Embedding`，北京智源人工智能研究院（`BAAI`）在`2023`年推出的`Bert`类通用文本嵌入模型。

`BGE`论文中，提出了`C-PACK`，一个中文`Embedding`领域的资源包，`C-Pack`包括三个关键资源：`C-MTEB`、`C-MTP`和`C-TEM`。
- `C-MTEB`为中文Embedding领域的benchmark；
- `C-MTP`为中文语料库中提取的成对数据集；
- `C-TEM`是一套涵盖多种参数规模的`Embedding`模型。

`BGE`还介绍了使用到的特殊训练方法。
- `RetroMAE`预训练（数据集：悟道、`Pile`）
- 通用文本上`fine-tune`（数据集：`C-MTP unlabeled`）
- 特定任务上`fine-tune`（数据集：`C-MTP labeled`）

`BGE`有`3`个参数版本，`small`（`24M`）、`base`（`102M`）、`large`（`326M`），还区分不同中英版本，已开源。

### **C-PACK**
#### **C-MTEB**
在过去的几年里提出的研究中文`Embedding`的几个数据集如`CMNLI`、`DuReader`等都是独立策划的，每个数据集只关注`Embedding`的一个特定功能。论文中提出的`C-MTEB`(`Chinese Massive Text Embedding Benchmark`)是为建立适当的`benchmark`，作为`MTEB`的中文扩展，可以全面评估中文`Embedding`的所有功能。`C-MTEB`包括`35`个公共可用数据集，数据集根据它们评估的`Embedding`的任务进行分类，分为`6`类，
- 检索任务(`Retrieval`)：对于每个查询，它在语料库中查找`Top-k`个相似的文档。
- 重排序(`Re-ranking`)：根据`Embedding`的相似度对候选文档重新排序。
- 语义文本相似度(`STS`)：基于两个句子的`Embedding`相似度来衡量它们的相关性。
- 分类任务(`Classification`)：基于`label`做输入`embedding`预测。
- 文本对分类(`Pair-classification`)：这个任务处理一对输入句子，它们的关系由Embedding相似度来预测。
- 聚类任务(`Clustering`)：聚类任务是将句子分成有意义的类。

我理解是按照定义这里的分类任务应该表述为回归任务。另外上述有几个任务可能不好区分，`STS`和`Pair-classification`都基于`Embedding`相似度来实现的。但`STS`是判断两个文本是否表达相似的语义，`Pair-classification`是判断两个文本是否具有一定的关系（如：标题与文章的关系）。

#### **C-MTP**
`C-MTP`(`Chinese Massive Text Pairs`)，是一个庞大的数据集，从`labeled`和`unlabeled`的中文语料库中提取，用于训练`Embedding`模型。
- `unlabeled`的数据主要来自网络，对于每一篇文章，提取(标题、段落)形成一个文本对。从网络和其他公共资源中挑选的文本对不能保证密切相关，因此，本文使用了一种简单的策略进行过滤：第三方模型`Text2VecChinese2`来对每个文本对的关系强度进行评分，并删除得分低于阈值的样本。
- `labeled`的数据使用的是`T2-Ranking`、`DuReader`等公开数据集，虽然它比`unlabeled`的数据少得多，但大多数数据都是从人工注释中整理出来的，从而确保了相关性的高可信度。

#### **C-TEM**
`C-TEM`(`Chinese Text Embedding Models`)，是一套涵盖多种参数规模的`Embedding`模型，有三种可选的参数规模：小型(`24M`)、基础(`102M`)和大型(`326M`)，它们为用户提供了在效率和有效性之间进行权衡的灵活性。`C-TEM`中的模型已经经过了良好的预训练，并且对各种任务具有很强的通用性。同时，如果将`Embedding`应用于特定场景,它们也可以进一步微调。

## **训练方法**
### **RetroMAE预训练**
`RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder`

基于掩码自编码器（`Masked Auto-Encoder`）的面向检索导向的预训练范式，用`Wudao`纯文本语料训练，利用了`RetroMAE`，重建污染的编码向量；

![Desktop View](/assets/img/20250615/bge_retromae_arch.png){: width="500" height="300" }
_RetroMAE预训练（图片来自网络）_

#### **关键设计**
- 训练时使用`mask`过的文本输入`encoder`，得到句子`embedding`；再次对文本输入`mask`后，与句子`embedding`拼接，共同输入`decoder`，得到重建后的句子。
- 非对称的模型结构，`encoder`拥有像`BERT`一样全尺寸的`transformer`，`decoder`只有一层的`transformer`。
- 非对称的掩码比例，`encoder`：`15%-30%`，`decoder`：`50%-70%`。

#### **核心思想**
`auto-encoding`对编码质量的要求更高，传统的自回归模型在解码过程中更关注前缀，而传统的`MLM`模型只掩盖一小部分(`15%`)的输入。相比之下，`RetroMAE`对解码过程中的大部分输入进行了激进的掩码，因此，重构不仅依赖解码器的输入，还更依赖句子嵌入，这将强制编码器深入捕捉输入的语义。
#### **具体方法**

![Desktop View](/assets/img/20250615/bge_retromae_pretrain.png){: width="500" height="300" }
_RetroMAE预训练方法（图片来自网络）_

**Encoding**
1. 随机用特殊`token`把输入文本`mask`掉，然后输入`BERT`（`12`层，`hidden_dim=768`）。
2. 最终输出的`[CLS] token`作为整句的`embedding`。
3. 像`BERT`一样预测被mask的token，计算损失，得到`𝐿𝑒𝑛𝑐`

**Decoding**
1. 再次对输入文本进行`mask`，然后将输入文本的`encoder`得到的句子`embedding`与输入文本的`embedding`进行拼接，送进单层`transformer`。
2. 使用所有被`mask`的`token`与真实词的交叉熵的和作为`decoding`阶段的损失`𝐿𝑑𝑒𝑐`。

**Enhanced Decoding**

这是`RetroMAE`论文最特殊的地方，作者认为原有的`decoding`策略得到的信息不够，主要体现在以下方面，
1. 交叉熵的计算仅仅基于被mask掉的那部分`token`。
2. 每个被`mask`的`token`在重建时都基于同一个上下文信息，也就是说在经过`decoder`得到一个`hidden state`后，所有`token`的重建都是基于这个`hidden state`的。

![Desktop View](/assets/img/20250615/bge_retromae_pretrain_enhance_decode.png){: width="500" height="300" }
_RetroMAE Enhance Decode（图片来自网络）_

基于此，提出了新的`enhanced decoding`，从输入的句子中得到更多的训练信号（训练信号就是让损失函数收敛的信号），根据不同的上下文执行重建任务。

通过双流注意力（`two-stream self-attention`，同时处理局部和全局信息）和特定位置注意掩码（`position-specific attention mask`，动态控制解码时可见的上下文范围），`M`掩码率很高，同时可以利用除了特殊`tokens`之外的所有`tokens`来训练模型，确保所有`token`参与重建过程，这样一来，训练语料的利用率很高，提升语义利用效率。

这种设计既增加了训练信号的多样性，又避免了传统`MLM`仅利用`15%`掩码`token`的局限性。

<font color="red">举个例子，详细解释下，</font>

假设输入的`token`序列长度为`𝑁−1`
1. 将`encoding`阶段获得的句子`embedding` (也就是前面得到的`[CLS] token`)重复`𝑁`份，变成长度为`𝑁`的序列，并加上位置编码，作为`decoder`的其中一个输入`𝐻1`。
2. 将`encoding`阶段获得的句子`embedding` ，和输入的`token`序列（没有`mask`）拼接，组成长度为`𝑁`的序列，并加上位置编码，作为`decoder`的第二个输入`𝐻2`。
3. `decoder`做`attention`计算时， `𝐻1`作为`𝑄`，`𝐻2`作为`𝐾`和`𝑉`。
4. 使用`mask`矩阵把`attention`矩阵`mask`掉，逻辑如下：每个`token`能看到的`token`都是通过采样得到的（但看不到自己），且都能看到首个`token`，因为首个`token`是`encoder`产出的句子embedding的信息。
5. 用`enhanced decoding`生成的向量做句子重建，使用所有被`mask`的`token`与真实词的交叉熵的和作为`decoding`阶段的损失`𝐿𝑑𝑒𝑐`。

**Loss**

训练`loss`即为上述的`𝐿𝑒𝑛𝑐+𝐿𝑑𝑒𝑐`。

### **通用文本上fine-tune**
用`C-MTP`无标签(`unlabeled`)数据集训练，对比学习从负样本中如何区分出成对的文本。
### **特定任务上fine-tune**
用`C-MTP`有监督(`labeled`)数据集训练，由于标签数据是多任务的，所以加入了指令微调实现多任务下的微调。
## **BAAI/bge-large-zh-v1.5**
举个例子，`bge-large-zh-v1.5`

[https://huggingface.co/BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)

## **MTEB测评榜单**
[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)
